{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f4c51d",
   "metadata": {},
   "source": [
    "[Authenticating to sovereign cloud](https://learn.microsoft.com/en-us/python/api/overview/azure/search-documents-readme?view=azure-python#authenticate-in-a-national-cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "056b4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using search endpoint https://avcoe-demo-ai-search-mcp-search.search.azure.us, and index: avcoe-demo-ai-search-mcp-index-and-vectorize\n",
      "Using authority host: login.microsoftonline.us\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, AzureAuthorityHosts\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "env_directory = Path.cwd() / \".azure\"\n",
    "\n",
    "env_file = next(\n",
    "    (candidate / \".env\" for candidate in env_directory.glob(\"avcoe-*\")),\n",
    "    None,\n",
    ")\n",
    "\n",
    "if env_file is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate an avcoe-* environment directory under .azure\"\n",
    "    )\n",
    "\n",
    "\n",
    "load_dotenv(dotenv_path=env_file, override=False)\n",
    "\n",
    "\n",
    "search_endpoint = os.environ[\"SEARCH_SERVICE_ENDPOINT\"]\n",
    "\n",
    "search_index = os.environ[\"SEARCH_INDEX_NAME\"]\n",
    "\n",
    "print(f\"Using search endpoint {search_endpoint}, and index: {search_index}\")\n",
    "\n",
    "authority_host = (\n",
    "    AzureAuthorityHosts.AZURE_GOVERNMENT\n",
    "    if os.getenv(\"CLOUD_NAME\") == \"AzureUSGovernment\"\n",
    "    else AzureAuthorityHosts.AZURE_PUBLIC_CLOUD\n",
    ")\n",
    "\n",
    "print(f\"Using authority host: {authority_host}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a0db437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using credential scope: https://search.azure.us\n"
     ]
    }
   ],
   "source": [
    "# Set the correct credential scope for Azure Government\n",
    "audience = (\"https://search.azure.us\" if os.getenv(\"CLOUD_NAME\") == \"AzureUSGovernment\"else \"https://search.azure.com\")\n",
    "\n",
    "print(f\"Using credential scope: {audience}\")\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Create SearchClient with the scoped credential\n",
    "search_client = SearchClient(search_endpoint, search_index, credential, audience=audience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc275151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': [{'value': 'llama.pdf', 'count': 61},\n",
       "  {'value': 'deepseek.pdf', 'count': 41}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results = search_client.search(\"*\",\n",
    "                                 facets=[\"title\"],\n",
    ")\n",
    "\n",
    "results.get_facets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "468834f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "InteractiveBrowserBrokerCredential.get_token_info failed: (pii). Status: Response_Status.Status_IncorrectConfiguration, Error code: 3399614475, Tag: 508634112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"chunk\": \"to the zero-shot setting. The CoT in few-shot\\nmay hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation\\nprotocols with default prompts provided by their creators. For code and math benchmarks, the\\nHumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,\\nC#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated\\nusing CoT format, with data collected between August 2024 and January 2025. The Codeforces\\ndataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,\\nafter which the expected ratings and percentages of competitors are calculated. SWE-Bench\\nverified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related\\nbenchmarks are measured using a \\\"diff\\\" format. DeepSeek-R1 outputs are capped at a maximum\\nof 32,768 tokens for each benchmark.\\n\\nBaselines We conduct comprehensive evaluations against several strong baselines, including\\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.\\nSince accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-\\nmance based on official reports. For distilled models, we also compare the open-source model\\nQwQ-32B-Preview (Qwen, 2024a).\\n\\nEvaluation Setup We set the maximum generation length to 32,768 tokens for the models.\\nWe found that using greedy decoding to evaluate long-output reasoning models results in\\nhigher repetition rates and significant variability across different checkpoints. Therefore, we\\ndefault to pass@\\ud835\\udc58 evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.\\nSpecifically, we use a sampling temperature of 0.6 and a top-\\ud835\\udc5d value of 0.95 to generate \\ud835\\udc58\\n\\nresponses (typically between 4 and 64, depending on the test set size) for each question. Pass@1\\nis then calculated as\\n\\npass@1 =\\n1\\n\\ud835\\udc58\\n\\n\\ud835\\udc58\\u2211\\ufe01\\n\\ud835\\udc56=1\\n\\n\\ud835\\udc5d\\ud835\\udc56,\\n\\nwhere \\ud835\\udc5d\\ud835\\udc56 denotes the correctness of the \\ud835\\udc56-th response.\",\n",
      "  \"title\": \"deepseek.pdf\",\n",
      "  \"@search.score\": 1.9834923,\n",
      "  \"@search.reranker_score\": 2.3151230812072754,\n",
      "  \"@search.highlights\": null,\n",
      "  \"@search.captions\": null\n",
      "}\n",
      "{\n",
      "  \"chunk\": \"higher repetition rates and significant variability across different checkpoints. Therefore, we\\ndefault to pass@\\ud835\\udc58 evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.\\nSpecifically, we use a sampling temperature of 0.6 and a top-\\ud835\\udc5d value of 0.95 to generate \\ud835\\udc58\\n\\nresponses (typically between 4 and 64, depending on the test set size) for each question. Pass@1\\nis then calculated as\\n\\npass@1 =\\n1\\n\\ud835\\udc58\\n\\n\\ud835\\udc58\\u2211\\ufe01\\n\\ud835\\udc56=1\\n\\n\\ud835\\udc5d\\ud835\\udc56,\\n\\nwhere \\ud835\\udc5d\\ud835\\udc56 denotes the correctness of the \\ud835\\udc56-th response. This method provides more reliable\\nperformance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang\\net al., 2022) using 64 samples, denoted as cons@64.\\n\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n\\n12\\n\\nhttps://aider.chat\\nhttps://codeforces.com\\nhttps://www.cms.org.cn/Home/comp/comp/cid/12.html\\n\\n\\n3.1. DeepSeek-R1 Evaluation\\n\\nBenchmark (Metric)\\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\\n\\nArchitecture - - MoE - - MoE\\n# Activated Params - - 37B - - 37B\\n# Total Params - - 671B - - 671B\\n\\nEnglish\\n\\nMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\\nMMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9\\nMMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0\\nDROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2\\nIF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3\\nGPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\\nSimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1\\nFRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5\\nAlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\\nArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\\n\\nCode\\n\\nLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\\nCodeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\\nCodeforces (Rating) 717 759 1134 1820 2061 2029\\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\\n\\nMath\\nAIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8\\nMATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3\",\n",
      "  \"title\": \"deepseek.pdf\",\n",
      "  \"@search.score\": 0.7683673,\n",
      "  \"@search.reranker_score\": 2.3118443489074707,\n",
      "  \"@search.highlights\": null,\n",
      "  \"@search.captions\": null\n",
      "}\n",
      "{\n",
      "  \"chunk\": \"annotation is not con-\\nducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward\\nhacking (Gao et al., 2022), and retraining the reward model needs additional training resources\\nand it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good\\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\\net al., 2024), its advantages are limited compared to the additional computational overhead it\\nintroduces during the large-scale reinforcement learning process in our experiments.\\n\\nMonte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-\\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\\ncompute scalability. This approach involves breaking answers into smaller parts to allow the\\nmodel to explore the solution space systematically. To facilitate this, we prompt the model to\\ngenerate multiple tags that correspond to specific reasoning steps necessary for the search. For\\ntraining, we first use collected prompts to find answers via MCTS guided by a pre-trained value\\nmodel. Subsequently, we use the resulting question-answer pairs to train both the actor model\\nand the value model, iteratively refining the process.\\n\\nHowever, this approach encounters several challenges when scaling up the training. First,\\nunlike chess, where the search space is relatively well-defined, token generation presents an\\n\\n15\\n\\n\\n\\nexponentially larger search space. To address this, we set a maximum extension limit for each\\nnode, but this can lead to the model getting stuck in local optima. Second, the value model\\ndirectly influences the quality of generation since it guides each step of the search process.\\nTraining a fine-grained value model is inherently difficult, which makes it challenging for the\\nmodel to iteratively improve. While AlphaGo\\u2019s core success relied on training a value model to\",\n",
      "  \"title\": \"deepseek.pdf\",\n",
      "  \"@search.score\": 1.7577097,\n",
      "  \"@search.reranker_score\": 2.2176883220672607,\n",
      "  \"@search.highlights\": null,\n",
      "  \"@search.captions\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "results = search_client.search(\"test\",\n",
    "                                 top=3,\n",
    "                                 include_total_count=True,\n",
    "                                 query_type=\"semantic\",\n",
    "                                 filter=\"title eq 'deepseek.pdf'\",\n",
    "                                 select=[\"title\", \"chunk\"],)\n",
    "                                 \n",
    "\n",
    "import json\n",
    "\n",
    "for result in results:\n",
    "    print(json.dumps(dict(result), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41452d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e56211",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "illegal target for annotation (3690852108.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[27], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"chunk_id\": \"da69dc6effcc_aHR0cHM6Ly9hdmNvZWRlbW9haXNlYXJjaG1jcHN0Zy5ibG9iLmNvcmUudXNnb3ZjbG91ZGFwaS5uZXQvYWlzZWFyY2hkYXRhL2RlZXBzZWVrLnBkZg2_pages_2\",\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m illegal target for annotation\n"
     ]
    }
   ],
   "source": [
    "  \"chunk_id\": \"da69dc6effcc_aHR0cHM6Ly9hdmNvZWRlbW9haXNlYXJjaG1jcHN0Zy5ibG9iLmNvcmUudXNnb3ZjbG91ZGFwaS5uZXQvYWlzZWFyY2hkYXRhL2RlZXBzZWVrLnBkZg2_pages_2\",\n",
    "  \"parent_id\": \"aHR0cHM6Ly9hdmNvZWRlbW9haXNlYXJjaG1jcHN0Zy5ibG9iLmNvcmUudXNnb3ZjbG91ZGFwaS5uZXQvYWlzZWFyY2hkYXRhL2RlZXBzZWVrLnBkZg2\",\n",
    "  \"chunk\": \"Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n\\n3.2 Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n\\n4 Discussion 14\\n\\n4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 14\\n\\n4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n\\n5 Conclusion, Limitations, and Future Work 16\\n\\nA Contributions and Acknowledgments 20\\n\\n2\\n\\n\\n\\n1. Introduction\\n\\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\\ntowards Artificial General Intelligence (AGI).\\n\\nRecently, post-training has emerged as an important component of the full training pipeline.\\nIt has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt\\nto user preferences, all while requiring relatively minimal computational resources against\\npre-training. In the context of reasoning capabilities, OpenAI\\u2019s o1 (OpenAI, 2024b) series models\\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\\nThought reasoning process. This approach has achieved significant improvements in various\\nreasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\\nof effective test-time scaling remains an open question for the research community. Several prior\\nworks have explored various approaches, including process-based reward models (Lightman\\net al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\\nand search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\\net al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning\\nperformance comparable to OpenAI\\u2019s o1 series models.\",\n",
    "  \"title\": \"deepseek.pdf\",\n",
    "  \"@search.score\": 1.7589658,\n",
    "  \"@search.reranker_score\": null,\n",
    "  \"@search.highlights\": null,\n",
    "  \"@search.captions\": null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028a41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
